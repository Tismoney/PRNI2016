{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "from reskit.norms import binar_norm, wbysqdist\n",
    "from reskit.norms import spectral_norm\n",
    "\n",
    "from reskit.features import degrees,  pagerank\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier \n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "from reskit.core import Transformer, Pipeliner\n",
    "\n",
    "def orig(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция считывания данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_autism(path_to_read='Data/dti/', distances=True):\n",
    "    def get_autism_distances(loc_name):\n",
    "        with open(loc_name, 'r') as f:\n",
    "            read_data = f.readlines()\n",
    "\n",
    "        read_data = pd.DataFrame(\n",
    "            np.array([np.array(item[:-1].split()).astype(int) for item in read_data]))\n",
    "\n",
    "        return read_data\n",
    "\n",
    "    def get_distance_matrix(coords):\n",
    "        if type(coords) == pd.core.frame.DataFrame:\n",
    "            coords = coords.values\n",
    "        elif type(coords) != np.ndarray:\n",
    "            print('Provide either pandas df or numpy array!')\n",
    "            return -1\n",
    "\n",
    "        shape = len(coords)\n",
    "        dist_matrix = np.zeros((shape, shape))\n",
    "        del shape\n",
    "        for i in range(len(coords)):\n",
    "            for j in range(i + 1, len(coords)):\n",
    "                dist_matrix[i, j] = np.linalg.norm(coords[i, :] - coords[j, :])\n",
    "                dist_matrix[j, i] = dist_matrix[i, j]\n",
    "        return dist_matrix\n",
    "\n",
    "    target_vector = []  # this will be a target vector (diagnosis)\n",
    "    matrices = []  # this will be a list of connectomes\n",
    "    all_files = sorted(os.listdir(path_to_read))\n",
    "    matrix_files = [\n",
    "        item for item in all_files if 'DTI_connectivity' in item and 'All' not in item]\n",
    "    distance_files = [\n",
    "        item for item in all_files if 'DTI_region_xyz_centers' in item and 'All' not in item]\n",
    "\n",
    "    # for each file in a sorted (!) list of files:\n",
    "    for filename in matrix_files:\n",
    "\n",
    "        A_dataframe = pd.read_csv(\n",
    "            path_to_read + filename, sep='   ', header=None, engine='python')\n",
    "        A = A_dataframe.values  # we will use a list of numpy arrays, NOT pandas dataframes\n",
    "        matrices.append(A)# append a matrix to our list\n",
    "        if \"ASD\" in filename:\n",
    "            target_vector.append(1)\n",
    "        elif \"TD\" in filename:\n",
    "            target_vector.append(0)\n",
    "    asd_dict = {}\n",
    "    asd_dict['X'] = np.array(matrices)\n",
    "    asd_dict['y'] = np.array(target_vector)\n",
    "    if distances:\n",
    "        dist_matrix_list = []\n",
    "        for item in distance_files:\n",
    "            # print(item)\n",
    "            cur_coord = get_autism_distances(path_to_read + item)\n",
    "            cur_dist_mtx = get_distance_matrix(cur_coord)\n",
    "            dist_matrix_list += [cur_dist_mtx]\n",
    "\n",
    "        asd_dict['dist'] = np.array(dist_matrix_list)\n",
    "\n",
    "    return asd_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция понижения ранга матрицы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "def matrix_nmf(data, k = 1):\n",
    "    new_data = {}\n",
    "    new_data['y'] = data['y']\n",
    "    new_data['dist'] = data['dist']\n",
    "    new_data['X'] = np.zeros(shape = (data['X'].shape[0], data['X'].shape[1], k))\n",
    "    model = NMF(n_components=k, init='random', random_state=0)\n",
    "    for i in np.arange(data['X'].shape[0]):\n",
    "        model.fit(data['X'][i])\n",
    "        new_data['X'][i] = model.components_.T\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сделаем один пайплайн"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_cv = StratifiedKFold(n_splits=10,\n",
    "                          shuffle=True,\n",
    "                          random_state=0)\n",
    "\n",
    "eval_cv = StratifiedKFold(n_splits=10,\n",
    "                          shuffle=True,\n",
    "                          random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = 'Data/dti/'\n",
    "data = Transformer(get_autism).fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = Transformer(matrix_nmf, {'data': data, 'k': 100}).fit_transform(data)\n",
    "data = Transformer(degrees, collect=['degrees']).fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[   7.52828664,   12.07418722,   20.85524477, ...,   36.55368452,\n",
      "          30.34175288,   96.93806036],\n",
      "       [   7.75500644,   12.66470406,   18.42257705, ...,   49.24504447,\n",
      "          34.95061284,   55.06679621],\n",
      "       [   9.55708306,    9.56270304,   16.08112848, ...,   36.76626794,\n",
      "          65.94660533,   80.20688142],\n",
      "       ..., \n",
      "       [   9.87012187,   12.96094476,   19.31119862, ...,   64.75001648,\n",
      "          35.64774145,   73.99864831],\n",
      "       [  10.43431905,   11.43522911,   16.84059489, ...,   61.71435862,\n",
      "          32.55247335,   36.66344232],\n",
      "       [  15.21932933,   10.94071944,   16.46274775, ...,  272.01793665,\n",
      "          25.52802062,   80.22287374]]), array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0]))\n"
     ]
    }
   ],
   "source": [
    "print data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(94, 100) (94,)\n"
     ]
    }
   ],
   "source": [
    "X, y = data \n",
    "print X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "steps = [('selector', VarianceThreshold()), ('scaler', MinMaxScaler()), ('classifier', LogisticRegression())] \n",
    "pipeline = Pipeline(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=10, random_state=0, shuffle=True),\n",
       "       error_score='raise',\n",
       "       estimator=Pipeline(steps=[('selector', VarianceThreshold(threshold=0.0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('classifier', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'classifier__penalty': ['l1', 'l2']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = dict(classifier__penalty=['l1', 'l2'])\n",
    "scoring = 'roc_auc'\n",
    "grid_clf = GridSearchCV(estimator=pipeline, param_grid=param_grid, scoring=scoring, n_jobs=-1, cv=grid_cv)\n",
    "grid_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.57900000000000007, 0.15293462655657808)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps[-1] = steps[-1][0], grid_clf.best_estimator_\n",
    "pipeline = Pipeline(steps)\n",
    "scores = cross_val_score(pipeline, X, y, scoring=scoring, cv=eval_cv, n_jobs=-1)\n",
    "np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Приведу некоторые результаты для различных К\n",
    "\n",
    "|       scores       |         std        |  k |\n",
    "|      :------:      |        :---:       | :-:|\n",
    "| 0.59233333333333338| 0.12568081264324588|  0 |\n",
    "| 0.59233333333333338| 0.12568081264324588|  1 | \n",
    "| 0.59233333333333338| 0.12568081264324588|  2 | \n",
    "| 0.59433333333333338| 0.13016271867679058| 10 |\n",
    "| 0.61133333333333328| 0.16110865898517063| 20 |\n",
    "| 0.63233333333333341| 0.16096272860510288| 30 |\n",
    "| 0.63233333333333341| 0.16096272860510288| 40 |\n",
    "| 0.63233333333333341| 0.16096272860510288| 45 |\n",
    "| 0.54400000000000004| 0.27115309328864384| 50 |\n",
    "| 0.21695468036742913| 0.22945079356294823| 75 |\n",
    "| 0.49466666666666664| 0.21695468036742913| 100|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Попробуем сделать это, используя класс Papiliner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь возникают проблемы, из-за того, что работая с один пайплайном мы явно можем задать парамеры функции. Тут же сделать это сложнее.\n",
    "\n",
    "Есть несколько решений:  \n",
    "1. Задавать k по дефолту в функции  \n",
    "2. Задавать параметр data функции matrix_eig через стороннюю переменную, расчитанную ранее\n",
    "\n",
    "На мой взгляд, лучши решением будет первый вариант. Поэтому на нем я и остановился"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def orig_vec(data):\n",
    "    matrices = []\n",
    "    for i in  data['X']:\n",
    "        matrices.append(np.hstack(i))\n",
    "    data['X_vec'] = matrices\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matrix_nmf_50(data, k = 50):\n",
    "    new_data = {}\n",
    "    new_data['y'] = data['y']\n",
    "    new_data['dist'] = data['dist']\n",
    "    new_data['X'] = np.zeros(shape = (data['X'].shape[0], data['X'].shape[1], k))\n",
    "    model = NMF(n_components=k, init='random', random_state=0)\n",
    "    for i in np.arange(data['X'].shape[0]):\n",
    "        model.fit(data['X'][i])\n",
    "        new_data['X'][i] = model.components_.T\n",
    "    return orig_vec(new_data)\n",
    "\n",
    "def matrix_nmf_100(data, k = 100):\n",
    "    new_data = {}\n",
    "    new_data['y'] = data['y']\n",
    "    new_data['dist'] = data['dist']\n",
    "    new_data['X'] = np.zeros(shape = (data['X'].shape[0], data['X'].shape[1], k))\n",
    "    model = NMF(n_components=k, init='random', random_state=0)\n",
    "    for i in np.arange(data['X'].shape[0]):\n",
    "        model.fit(data['X'][i])\n",
    "        new_data['X'][i] = model.components_.T\n",
    "    return orig_vec(new_data)\n",
    "\n",
    "def matrix_nmf_150(data, k = 150):\n",
    "    new_data = {}\n",
    "    new_data['y'] = data['y']\n",
    "    new_data['dist'] = data['dist']\n",
    "    new_data['X'] = np.zeros(shape = (data['X'].shape[0], data['X'].shape[1], k))\n",
    "    model = NMF(n_components=k, init='random', random_state=0)\n",
    "    for i in np.arange(data['X'].shape[0]):\n",
    "        model.fit(data['X'][i])\n",
    "        new_data['X'][i] = model.components_.T\n",
    "    return orig_vec(new_data)\n",
    "\n",
    "def matrix_nmf_200(data, k = 200):\n",
    "    new_data = {}\n",
    "    new_data['y'] = data['y']\n",
    "    new_data['dist'] = data['dist']\n",
    "    new_data['X'] = np.zeros(shape = (data['X'].shape[0], data['X'].shape[1], k))\n",
    "    model = NMF(n_components=k, init='random', random_state=0)\n",
    "    for i in np.arange(data['X'].shape[0]):\n",
    "        model.fit(data['X'][i])\n",
    "        new_data['X'][i] = model.components_.T\n",
    "    return orig_vec(new_data)\n",
    "\n",
    "def matrix_nmf_250(data, k = 250):\n",
    "    new_data = {}\n",
    "    new_data['y'] = data['y']\n",
    "    new_data['dist'] = data['dist']\n",
    "    new_data['X'] = np.zeros(shape = (data['X'].shape[0], data['X'].shape[1], k))\n",
    "    model = NMF(n_components=k, init='random', random_state=0)\n",
    "    for i in np.arange(data['X'].shape[0]):\n",
    "        model.fit(data['X'][i])\n",
    "        new_data['X'][i] = model.components_.T\n",
    "    return orig_vec(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Weighters</th>\n",
       "      <th>Normalizers</th>\n",
       "      <th>Featurizers</th>\n",
       "      <th>Selectors</th>\n",
       "      <th>Scalers</th>\n",
       "      <th>Classifiers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UCLAsource</td>\n",
       "      <td>origW</td>\n",
       "      <td>origN</td>\n",
       "      <td>nmf_50</td>\n",
       "      <td>var_threshold</td>\n",
       "      <td>minmax</td>\n",
       "      <td>LR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UCLAsource</td>\n",
       "      <td>origW</td>\n",
       "      <td>origN</td>\n",
       "      <td>nmf_100</td>\n",
       "      <td>var_threshold</td>\n",
       "      <td>minmax</td>\n",
       "      <td>LR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UCLAsource</td>\n",
       "      <td>origW</td>\n",
       "      <td>origN</td>\n",
       "      <td>nmf_150</td>\n",
       "      <td>var_threshold</td>\n",
       "      <td>minmax</td>\n",
       "      <td>LR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UCLAsource</td>\n",
       "      <td>origW</td>\n",
       "      <td>origN</td>\n",
       "      <td>nmf_200</td>\n",
       "      <td>var_threshold</td>\n",
       "      <td>minmax</td>\n",
       "      <td>LR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UCLAsource</td>\n",
       "      <td>origW</td>\n",
       "      <td>origN</td>\n",
       "      <td>nmf_250</td>\n",
       "      <td>var_threshold</td>\n",
       "      <td>minmax</td>\n",
       "      <td>LR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Data Weighters Normalizers Featurizers      Selectors Scalers  \\\n",
       "0  UCLAsource     origW       origN      nmf_50  var_threshold  minmax   \n",
       "1  UCLAsource     origW       origN     nmf_100  var_threshold  minmax   \n",
       "2  UCLAsource     origW       origN     nmf_150  var_threshold  minmax   \n",
       "3  UCLAsource     origW       origN     nmf_200  var_threshold  minmax   \n",
       "4  UCLAsource     origW       origN     nmf_250  var_threshold  minmax   \n",
       "\n",
       "  Classifiers  \n",
       "0          LR  \n",
       "1          LR  \n",
       "2          LR  \n",
       "3          LR  \n",
       "4          LR  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv = StratifiedKFold(n_splits=10,\n",
    "                          shuffle=True,\n",
    "                          random_state=0)\n",
    "\n",
    "eval_cv = StratifiedKFold(n_splits=10,\n",
    "                          shuffle=True,\n",
    "                          random_state=1)\n",
    "\n",
    "data = [('UCLAsource', Transformer(get_autism)),\n",
    "        #('UCLAbaseline', Transformer(get_baseline))\n",
    "       ]\n",
    "\n",
    "#Only low_rank\n",
    "weighters = [('origW', Transformer(orig)),\n",
    "             #('binar', Transformer(binar_norm)),\n",
    "             #('wbysqdist', Transformer(wbysqdist)),\n",
    "             #('low_rank', Transformer(matrix_eig_k))\n",
    "            ]\n",
    "\n",
    "\n",
    "normalizers = [('origN', Transformer(orig)),\n",
    "               #('spectral', Transformer(spectral_norm))\n",
    "              ]\n",
    "\n",
    "featurizers = [#('origF', Transformer(orig_vec, collect=['X_vec'])),\n",
    "               #('degrees', Transformer(degrees, collect=['degrees']))\n",
    "               ('nmf_50', Transformer(matrix_nmf_50, collect=['X_vec'])),\n",
    "               ('nmf_100', Transformer(matrix_nmf_100, collect=['X_vec'])),\n",
    "               ('nmf_150', Transformer(matrix_nmf_150, collect=['X_vec'])),\n",
    "               ('nmf_200', Transformer(matrix_nmf_200, collect=['X_vec'])),\n",
    "               ('nmf_250', Transformer(matrix_nmf_250, collect=['X_vec']))]\n",
    "\n",
    "selectors = [('var_threshold', VarianceThreshold())]\n",
    "\n",
    "scalers = [('minmax', MinMaxScaler()),\n",
    "           ('origS', FunctionTransformer(orig))]\n",
    "\n",
    "#For tests, don`t use XGB, it needs a lot of time\n",
    "classifiers = [('LR', LogisticRegression()),\n",
    "               #('RF', RandomForestClassifier()),\n",
    "               #('SVC', SVC()),\n",
    "               #('XGB', XGBClassifier(nthread=1)),\n",
    "               #('SGD', SGDClassifier())\n",
    "              ]\n",
    "\n",
    "steps = [('Data', data),\n",
    "         ('Weighters', weighters),\n",
    "         ('Normalizers', normalizers),\n",
    "         ('Featurizers', featurizers),\n",
    "         ('Selectors', selectors),\n",
    "         ('Scalers', scalers),\n",
    "         ('Classifiers', classifiers)]\n",
    "\n",
    "banned_combos = [#('UCLAsource', 'origN'),\n",
    "                 #('UCLAsource', 'origF'),\n",
    "                 ('UCLAbaseline', 'degrees'),\n",
    "                 ('UCLAbaseline', 'binar'),\n",
    "                 ('UCLAbaseline', 'wbysqdist'),\n",
    "                 ('UCLAbaseline', 'spectral'),\n",
    "                 ('UCLAbaseline', 'low_rank'),\n",
    "                 ('LR', 'origS'),\n",
    "                 ('SVC', 'origS'),\n",
    "                 ('SGD', 'origS'),\n",
    "                 ('RF', 'minmax'),\n",
    "                 ('XGB', 'minmax')]\n",
    "\n",
    "param_grid = dict(\n",
    "    LR=dict(\n",
    "        C=[0.01, 0.05, 0.1] + [0.05*i for i in range(3, 21)],\n",
    "        max_iter=[50, 100, 500],\n",
    "        penalty=['l1', 'l2']\n",
    "    ),\n",
    "    SGD=dict(\n",
    "        alpha=[0.001, 0.01, 0.1, 0.5, 1.0],\n",
    "        l1_ratio=[0, 0.2, 0.4, 0.6, 0.8, 1],\n",
    "        loss=['hinge', 'log', 'modified_huber'],\n",
    "        n_iter=[50, 100, 200],\n",
    "        penalty=['elasticnet']\n",
    "    ),\n",
    "    SVC=dict(\n",
    "        C=[0.0005, 0.001, 0.005, 0.01] + [i*0.05 for i in range(1,11)],\n",
    "        degree=[2, 3, 4],\n",
    "        kernel=['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        max_iter=[50, 100, 150],\n",
    "    ),\n",
    "    RF=dict(\n",
    "        criterion=['entropy', 'gini'],\n",
    "        max_depth=[3, 5, 7, 10, 20],\n",
    "        max_features=['log2', 'sqrt'] + [0.001, 0.005, 0.01, 0.05, 0.1, 0.25, 0.5, 1.0],\n",
    "        n_estimators=[10, 50, 100, 200, 500]\n",
    "    ),\n",
    "    XGB=dict(\n",
    "        colsample_bytree=[0.01] + [0.05*i for i in range(1,21)],\n",
    "        learning_rate=[0.01*i for i in range(1,6)] + [0.05*i for i in range(2,11)],\n",
    "        max_depth=[i for i in range(1,12)],\n",
    "        n_estimators=[10, 50, 100, 200, 500],\n",
    "        nthread=[1],\n",
    "        reg_alpha=[0, 1],\n",
    "        reg_lambda=[0, 1],\n",
    "        subsample=[0.5, 0.7, 1]\n",
    "    )\n",
    ")\n",
    "\n",
    "pipe = Pipeliner(steps, eval_cv=eval_cv, grid_cv=grid_cv, param_grid=param_grid, banned_combos=banned_combos)\n",
    "pipe.plan_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed previous results file -- results.csv.\n",
      "Line: 1/5\n",
      "Line: 2/5\n",
      "Line: 3/5\n",
      "Line: 4/5\n",
      "Line: 5/5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Weighters</th>\n",
       "      <th>Normalizers</th>\n",
       "      <th>Featurizers</th>\n",
       "      <th>Selectors</th>\n",
       "      <th>Scalers</th>\n",
       "      <th>Classifiers</th>\n",
       "      <th>grid_roc_auc_mean</th>\n",
       "      <th>grid_roc_auc_std</th>\n",
       "      <th>grid_roc_auc_best_params</th>\n",
       "      <th>eval_roc_auc_mean</th>\n",
       "      <th>eval_roc_auc_std</th>\n",
       "      <th>eval_roc_auc_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UCLAsource</td>\n",
       "      <td>origW</td>\n",
       "      <td>origN</td>\n",
       "      <td>nmf_50</td>\n",
       "      <td>var_threshold</td>\n",
       "      <td>minmax</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.158383</td>\n",
       "      <td>{'penalty': 'l2', 'C': 0.01, 'max_iter': 50}</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.255423</td>\n",
       "      <td>[ 0.2   0.48  0.8   0.45  0.2   0.2   0.55  0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UCLAsource</td>\n",
       "      <td>origW</td>\n",
       "      <td>origN</td>\n",
       "      <td>nmf_100</td>\n",
       "      <td>var_threshold</td>\n",
       "      <td>minmax</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.562145</td>\n",
       "      <td>0.132548</td>\n",
       "      <td>{'penalty': 'l1', 'C': 0.30000000000000004, 'm...</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.0889326</td>\n",
       "      <td>[ 0.6   0.52  0.54  0.5   0.4   0.75  0.55  0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UCLAsource</td>\n",
       "      <td>origW</td>\n",
       "      <td>origN</td>\n",
       "      <td>nmf_150</td>\n",
       "      <td>var_threshold</td>\n",
       "      <td>minmax</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.562057</td>\n",
       "      <td>0.152575</td>\n",
       "      <td>{'penalty': 'l2', 'C': 0.05, 'max_iter': 50}</td>\n",
       "      <td>0.563667</td>\n",
       "      <td>0.181698</td>\n",
       "      <td>[ 0.36666667  0.52        0.4         0.85    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UCLAsource</td>\n",
       "      <td>origW</td>\n",
       "      <td>origN</td>\n",
       "      <td>nmf_200</td>\n",
       "      <td>var_threshold</td>\n",
       "      <td>minmax</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.525355</td>\n",
       "      <td>0.138353</td>\n",
       "      <td>{'penalty': 'l2', 'C': 0.01, 'max_iter': 50}</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.16947</td>\n",
       "      <td>[ 0.6   0.76  0.44  0.4   0.25  0.8   0.5   0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UCLAsource</td>\n",
       "      <td>origW</td>\n",
       "      <td>origN</td>\n",
       "      <td>nmf_250</td>\n",
       "      <td>var_threshold</td>\n",
       "      <td>minmax</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.639184</td>\n",
       "      <td>0.242798</td>\n",
       "      <td>{'penalty': 'l2', 'C': 0.01, 'max_iter': 50}</td>\n",
       "      <td>0.588667</td>\n",
       "      <td>0.287326</td>\n",
       "      <td>[ 0.66666667  0.4         0.92        0.6     ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Data Weighters Normalizers Featurizers      Selectors Scalers  \\\n",
       "0  UCLAsource     origW       origN      nmf_50  var_threshold  minmax   \n",
       "1  UCLAsource     origW       origN     nmf_100  var_threshold  minmax   \n",
       "2  UCLAsource     origW       origN     nmf_150  var_threshold  minmax   \n",
       "3  UCLAsource     origW       origN     nmf_200  var_threshold  minmax   \n",
       "4  UCLAsource     origW       origN     nmf_250  var_threshold  minmax   \n",
       "\n",
       "  Classifiers grid_roc_auc_mean grid_roc_auc_std  \\\n",
       "0          LR               0.6         0.158383   \n",
       "1          LR          0.562145         0.132548   \n",
       "2          LR          0.562057         0.152575   \n",
       "3          LR          0.525355         0.138353   \n",
       "4          LR          0.639184         0.242798   \n",
       "\n",
       "                            grid_roc_auc_best_params eval_roc_auc_mean  \\\n",
       "0       {'penalty': 'l2', 'C': 0.01, 'max_iter': 50}             0.493   \n",
       "1  {'penalty': 'l1', 'C': 0.30000000000000004, 'm...             0.571   \n",
       "2       {'penalty': 'l2', 'C': 0.05, 'max_iter': 50}          0.563667   \n",
       "3       {'penalty': 'l2', 'C': 0.01, 'max_iter': 50}              0.53   \n",
       "4       {'penalty': 'l2', 'C': 0.01, 'max_iter': 50}          0.588667   \n",
       "\n",
       "  eval_roc_auc_std                                eval_roc_auc_scores  \n",
       "0         0.255423  [ 0.2   0.48  0.8   0.45  0.2   0.2   0.55  0....  \n",
       "1        0.0889326  [ 0.6   0.52  0.54  0.5   0.4   0.75  0.55  0....  \n",
       "2         0.181698  [ 0.36666667  0.52        0.4         0.85    ...  \n",
       "3          0.16947  [ 0.6   0.76  0.44  0.4   0.25  0.8   0.5   0....  \n",
       "4         0.287326  [ 0.66666667  0.4         0.92        0.6     ...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.get_results('Data/dti/', caching_steps=['Data', 'Weighters', 'Normalizers', 'Featurizers'], scoring=['roc_auc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
