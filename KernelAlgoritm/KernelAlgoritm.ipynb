{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подключим необходимые библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "from reskit.norms import binar_norm, wbysqdist\n",
    "from reskit.norms import spectral_norm\n",
    "\n",
    "from reskit.features import degrees,  pagerank\n",
    "\n",
    "from reskit.core import Transformer, Pipeliner\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier \n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "def orig(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция считывания данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_autism(path_to_read='Data/dti/', distances=True):\n",
    "    def get_autism_distances(loc_name):\n",
    "        with open(loc_name, 'r') as f:\n",
    "            read_data = f.readlines()\n",
    "\n",
    "        read_data = pd.DataFrame(\n",
    "            np.array([np.array(item[:-1].split()).astype(int) for item in read_data]))\n",
    "\n",
    "        return read_data\n",
    "\n",
    "    def get_distance_matrix(coords):\n",
    "        if type(coords) == pd.core.frame.DataFrame:\n",
    "            coords = coords.values\n",
    "        elif type(coords) != np.ndarray:\n",
    "            print('Provide either pandas df or numpy array!')\n",
    "            return -1\n",
    "\n",
    "        shape = len(coords)\n",
    "        dist_matrix = np.zeros((shape, shape))\n",
    "        del shape\n",
    "        for i in range(len(coords)):\n",
    "            for j in range(i + 1, len(coords)):\n",
    "                dist_matrix[i, j] = np.linalg.norm(coords[i, :] - coords[j, :])\n",
    "                dist_matrix[j, i] = dist_matrix[i, j]\n",
    "        return dist_matrix\n",
    "\n",
    "    target_vector = []  # this will be a target vector (diagnosis)\n",
    "    matrices = []  # this will be a list of connectomes\n",
    "    all_files = sorted(os.listdir(path_to_read))\n",
    "    matrix_files = [\n",
    "        item for item in all_files if 'DTI_connectivity' in item and 'All' not in item]\n",
    "    distance_files = [\n",
    "        item for item in all_files if 'DTI_region_xyz_centers' in item and 'All' not in item]\n",
    "\n",
    "    # for each file in a sorted (!) list of files:\n",
    "    for filename in matrix_files:\n",
    "\n",
    "        A_dataframe = pd.read_csv(\n",
    "            path_to_read + filename, sep='   ', header=None, engine='python')\n",
    "        A = A_dataframe.values  # we will use a list of numpy arrays, NOT pandas dataframes\n",
    "        matrices.append(A)# append a matrix to our list\n",
    "        if \"ASD\" in filename:\n",
    "            target_vector.append(1)\n",
    "        elif \"TD\" in filename:\n",
    "            target_vector.append(0)\n",
    "    asd_dict = {}\n",
    "    asd_dict['X'] = np.array(matrices)\n",
    "    asd_dict['y'] = np.array(target_vector)\n",
    "    if distances:\n",
    "        dist_matrix_list = []\n",
    "        for item in distance_files:\n",
    "            # print(item)\n",
    "            cur_coord = get_autism_distances(path_to_read + item)\n",
    "            cur_dist_mtx = get_distance_matrix(cur_coord)\n",
    "            dist_matrix_list += [cur_dist_mtx]\n",
    "\n",
    "        asd_dict['dist'] = np.array(dist_matrix_list)\n",
    "\n",
    "    return asd_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция понижения ранга матрицы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Для первоночальной настройки ядра\n",
    "\n",
    "from math import exp\n",
    "\n",
    "def low(A, k):\n",
    "    curs, vecs = np.linalg.eig(A)\n",
    "    curs_abs = abs(curs)\n",
    "    indeces_del = curs_abs.argsort()[:k]\n",
    "    vecs_n = np.delete(vecs, indeces_del, axis=1)\n",
    "    curs = np.delete(curs, indeces_del)\n",
    "    #vecs_i = np.delete(np.linalg.inv(vecs), indeces_del, axis=0)\n",
    "    A = vecs_n.dot(np.diag(curs)).astype('float')\n",
    "    return A\n",
    "\n",
    "def ker(data, i, j, k = 30):\n",
    "    A = low(data['X'][i], k)\n",
    "    B = low(data['X'][j], k)\n",
    "    d = np.linalg.norm(A.dot(A.T) - B.dot(B.T), 'fro')\n",
    "    #print \"A: {}, B: {}, d = {}\".format(A.shape, B.shape, d)\n",
    "    return exp(-d * 10**-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dist = np.zeros((94, 94), dtype = 'float')\n",
    "\n",
    "#%%time\n",
    "#for i in range(94):\n",
    "#    for j in range(i+1):\n",
    "#       dist[i][j] = ker(data, i, j, 255)\n",
    "#        dist[j][i] = dist[i][j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Обучим SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matrix_eig(data, k = 30):\n",
    "    #new_data = {}\n",
    "    #new_data['y'] = data['y']\n",
    "    #new_data['dist'] = data['dist']\n",
    "    #new_data['X'] = np.zeros(shape = (data['X'].shape[0], data['X'].shape[1], data['X'].shape[1] - k))\n",
    "    for i in np.arange(data['X'].shape[0]):\n",
    "        curs, vecs = np.linalg.eig(data['X'][i])\n",
    "        curs_abs = abs(curs)\n",
    "        indeces_del = curs_abs.argsort()[:k]\n",
    "        vecs_n = np.delete(vecs, indeces_del, axis=1)\n",
    "        curs = np.delete(curs, indeces_del)\n",
    "        vecs_i = np.delete(np.linalg.inv(vecs), indeces_del, axis=0)\n",
    "        data['X'][i] = vecs_n.dot(np.diag(curs)).dot(vecs_i).astype('float')\n",
    "    return orig_vec(data)\n",
    "\n",
    "def orig_vec(data):\n",
    "    matrix = []\n",
    "    for i in  data['X']:\n",
    "        matrix.append(np.hstack(i))\n",
    "    data['X_vec'] = matrix\n",
    "    return data\n",
    "\n",
    "def convert(A, mode, size = 264):\n",
    "    if mode == 'mat2vec':\n",
    "        A_vec = np.hstack(A)\n",
    "        return A_vec\n",
    "        \n",
    "    if mode == 'vec2mat':\n",
    "        A_mat = []\n",
    "        i = 0\n",
    "        while i != A.shape[0]:\n",
    "            A_str = A[0+i:size+i]\n",
    "            A_mat.append(A_str)\n",
    "            i += size\n",
    "        A_mat = np.vstack(A_mat)\n",
    "        return A_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Ядро\n",
    "\n",
    "def my_kernel(A, B):\n",
    "    A = convert(A, 'vec2mat')\n",
    "    B = convert(B, 'vec2mat')\n",
    "    d = np.linalg.norm(A.dot(A.T) - B.dot(B.T), 'fro')\n",
    "    return exp(-d * 10**-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обычным способом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = orig_vec(get_autism())\n",
    "X = data['X_vec']\n",
    "y = data['y']\n",
    "X_low = matrix_eig(data, k=255)['X_vec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example: X: [  0.  23.  26. ...,   0.  61.   0.]((69696,)), y: 1\n"
     ]
    }
   ],
   "source": [
    "print \"example: X: {}({}), y: {}\".format(X[0],X[0].shape, y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example: X: [  1.06302133   0.49439527   1.61100051 ...,   4.10690851  15.40864896\n",
      "  10.46470533]((69696,)), y: 1\n"
     ]
    }
   ],
   "source": [
    "print \"example: X: {}({}), y: {}\".format(X_low[0],X_low[0].shape, y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = SVC(kernel=my_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf.fit(X_low, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Используя класс Papiliner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Weighters</th>\n",
       "      <th>Normalizers</th>\n",
       "      <th>Featurizers</th>\n",
       "      <th>Selectors</th>\n",
       "      <th>Scalers</th>\n",
       "      <th>Classifiers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UCLAsource</td>\n",
       "      <td>origW</td>\n",
       "      <td>origN</td>\n",
       "      <td>low_rank_255</td>\n",
       "      <td>var_threshold</td>\n",
       "      <td>minmax</td>\n",
       "      <td>SVC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Data Weighters Normalizers   Featurizers      Selectors Scalers  \\\n",
       "0  UCLAsource     origW       origN  low_rank_255  var_threshold  minmax   \n",
       "\n",
       "  Classifiers  \n",
       "0         SVC  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv = StratifiedKFold(n_splits=10,\n",
    "                          shuffle=True,\n",
    "                          random_state=0)\n",
    "\n",
    "eval_cv = StratifiedKFold(n_splits=10,\n",
    "                          shuffle=True,\n",
    "                          random_state=1)\n",
    "\n",
    "data = [('UCLAsource', Transformer(get_autism))]\n",
    "\n",
    "weighters = [('origW', Transformer(orig)),\n",
    "             #('binar', Transformer(binar_norm))\n",
    "            ]\n",
    "\n",
    "\n",
    "normalizers = [('origN', Transformer(orig)),\n",
    "               #('spectral', Transformer(spectral_norm))\n",
    "              ]\n",
    "\n",
    "featurizers = [#('origF', Transformer(orig_vec, collect=['X_vec'])),\n",
    "               ('low_rank_255', Transformer(matrix_eig, collect=['X_vec']))]\n",
    "\n",
    "selectors = [('var_threshold', VarianceThreshold())]\n",
    "\n",
    "scalers = [('minmax', MinMaxScaler()),\n",
    "           ('origS', FunctionTransformer(orig))]\n",
    "\n",
    "\n",
    "classifiers = [('SVC', SVC())]\n",
    "\n",
    "steps = [('Data', data),\n",
    "         ('Weighters', weighters),\n",
    "         ('Normalizers', normalizers),\n",
    "         ('Featurizers', featurizers),\n",
    "         ('Selectors', selectors),\n",
    "         ('Scalers', scalers),\n",
    "         ('Classifiers', classifiers)]\n",
    "\n",
    "banned_combos = [\n",
    "                 ('LR', 'origS'),\n",
    "                 ('SVC', 'origS'),\n",
    "                 ('SGD', 'origS'),\n",
    "                 ('RF', 'minmax'),\n",
    "                 ('XGB', 'minmax')\n",
    "                 ]\n",
    "\n",
    "param_grid = dict(\n",
    "    SVC=dict(\n",
    "        C=[0.001, 0.01] + [i*0.1 for i in range(1,6)],\n",
    "        degree=[2, 3, 4],\n",
    "        kernel=[my_kernel],\n",
    "        max_iter=[50, 100, 150],\n",
    "    )\n",
    ")\n",
    "\n",
    "pipe = Pipeliner(steps, eval_cv=eval_cv, grid_cv=grid_cv, param_grid=param_grid, banned_combos=banned_combos)\n",
    "pipe.plan_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed previous results file -- results.csv.\n",
      "Line: 1/1\n"
     ]
    }
   ],
   "source": [
    "result = pipe.get_results('Data/dti/', caching_steps=['Data', 'Weighters', 'Normalizers', 'Featurizers'], scoring=['roc_auc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
