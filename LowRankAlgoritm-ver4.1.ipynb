{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подключим необходимые библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "from reskit.norms import binar_norm, wbysqdist\n",
    "from reskit.norms import spectral_norm\n",
    "\n",
    "from reskit.features import degrees,  pagerank\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier \n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from reskit.core import Transformer, Pipeliner\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "def orig(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import scipy.io\n",
    "import scipy.sparse\n",
    "import scipy.stats\n",
    "from scipy import interp\n",
    "import time\n",
    "import networkx as nx\n",
    "import sys\n",
    "import igraph as ig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция считывания данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_autism(path_to_read='Data/dti/', distances=True):\n",
    "    def get_autism_distances(loc_name):\n",
    "        with open(loc_name, 'r') as f:\n",
    "            read_data = f.readlines()\n",
    "\n",
    "        read_data = pd.DataFrame(\n",
    "            np.array([np.array(item[:-1].split()).astype(int) for item in read_data]))\n",
    "\n",
    "        return read_data\n",
    "\n",
    "    def get_distance_matrix(coords):\n",
    "        if type(coords) == pd.core.frame.DataFrame:\n",
    "            coords = coords.values\n",
    "        elif type(coords) != np.ndarray:\n",
    "            print('Provide either pandas df or numpy array!')\n",
    "            return -1\n",
    "\n",
    "        shape = len(coords)\n",
    "        dist_matrix = np.zeros((shape, shape))\n",
    "        del shape\n",
    "        for i in range(len(coords)):\n",
    "            for j in range(i + 1, len(coords)):\n",
    "                dist_matrix[i, j] = np.linalg.norm(coords[i, :] - coords[j, :])\n",
    "                dist_matrix[j, i] = dist_matrix[i, j]\n",
    "        return dist_matrix\n",
    "\n",
    "    target_vector = []  # this will be a target vector (diagnosis)\n",
    "    matrices = []  # this will be a list of connectomes\n",
    "    all_files = sorted(os.listdir(path_to_read))\n",
    "    matrix_files = [\n",
    "        item for item in all_files if 'DTI_connectivity' in item and 'All' not in item]\n",
    "    distance_files = [\n",
    "        item for item in all_files if 'DTI_region_xyz_centers' in item and 'All' not in item]\n",
    "\n",
    "    # for each file in a sorted (!) list of files:\n",
    "    for filename in matrix_files:\n",
    "\n",
    "        A_dataframe = pd.read_csv(\n",
    "            path_to_read + filename, sep='   ', header=None, engine='python')\n",
    "        A = A_dataframe.values  # we will use a list of numpy arrays, NOT pandas dataframes\n",
    "        matrices.append(A)# append a matrix to our list\n",
    "        if \"ASD\" in filename:\n",
    "            target_vector.append(1)\n",
    "        elif \"TD\" in filename:\n",
    "            target_vector.append(0)\n",
    "    asd_dict = {}\n",
    "    asd_dict['X'] = np.array(matrices)\n",
    "    asd_dict['y'] = np.array(target_vector)\n",
    "    if distances:\n",
    "        dist_matrix_list = []\n",
    "        for item in distance_files:\n",
    "            # print(item)\n",
    "            cur_coord = get_autism_distances(path_to_read + item)\n",
    "            cur_dist_mtx = get_distance_matrix(cur_coord)\n",
    "            dist_matrix_list += [cur_dist_mtx]\n",
    "\n",
    "        asd_dict['dist'] = np.array(dist_matrix_list)\n",
    "\n",
    "    return asd_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сделаем один пайплайн"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция понижения ранга матрицы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matrix_eig(data, k = 30):\n",
    "    data['X_low'] = np.zeros_like(data['X'])\n",
    "    for i in np.arange(data['X'].shape[0]):\n",
    "        curs, vecs = np.linalg.eig(data['X'][i])\n",
    "        curs_abs = abs(curs)\n",
    "        indeces_del = curs_abs.argsort()[:k]\n",
    "        vecs_n = np.delete(vecs, indeces_del, axis=1)\n",
    "        curs = np.delete(curs, indeces_del)\n",
    "        vecs_i = np.delete(np.linalg.inv(vecs), indeces_del, axis=0)\n",
    "        data['X_low'][i] = vecs_n.dot(np.diag(curs)).dot(vecs_i).astype('float')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The function accepts two lists of matrices\n",
    "#Matrices in the first list are symmetric adjacency matrices to work with\n",
    "#Matrices in the second list are the respective metrices of shortest path lengths (in the matching order!)\n",
    "\n",
    "#Note: matrices of shortest path lengths are optional, can be computed if nothing is provided\n",
    "#Note 2: It is assumed that weights in the adjacency matrix are proportional to strenght (inversely proportional\n",
    "#        to distances)\n",
    "\n",
    "#Returns: Pandas dataframe of computed metrics of shape N X (20 + 13*n_of_nodes): 20 graph-level metrics and 13 \n",
    "#         node-level metrics.\n",
    "#         List of long feature names\n",
    "\n",
    "#Known issues:\n",
    "\n",
    "# 1. Formally, some metrics can be switched off when the function is called. In fact, this can break \n",
    "#    some dependent functions. For example, if local_efficiency is set to false, graph_local_efficiency\n",
    "#    will not be computed (this is the average of local efficiencies). Another example: triangles use clustering\n",
    "#    coefficients and cannot be computed with clustering_coefficient=False\n",
    "\n",
    "# 2. Summary of node metrics (percentiles, stds, etc) is not computed within this function, except meaningful\n",
    "#    graph metrics that are averages of node-level metrics (these are: graph characteristic path length,\n",
    "#    graph global efficiency, graph local efficiency (two versions), graph clustering coefficient). Centralities \n",
    "#    are summarized in a single index using Freeman centralization formula (done for degree, closeness, \n",
    "#    betweenness and eigenvector centralities)\n",
    "\n",
    "# 3. It is assumed that the input graphs are weighted. No check for this is implemented, and some algorithms\n",
    "#    could be more efficient for binary adjacency matrices. At this stage, it did not seem to be worth trouble to \n",
    "#    implement two different functions for weighted and unweighted input graphs.\n",
    "\n",
    "\n",
    "\n",
    "def symm_metrics(list_of_matrices, #accepts list of matrices\n",
    "                 list_of_shortest_path_length_matrices = None, #accepts list of the respective SPL matrices\n",
    "                 \n",
    "                 #node metrics:\n",
    "                 degree=True, #weighted degrees\n",
    "                 neighborhood_degree=True, #average neighbor degrees (weighted)\n",
    "                 closeness_centrality=True, #inverse of the characteristic path length\n",
    "                 betweenness_centrality=True, \n",
    "                 eigenvector_centrality=True, \n",
    "                 clustering_coefficient=True, \n",
    "                 triangles=True, #weighted number of triangles\n",
    "                 eccentricity = True, #max shortest path between this vertex and other verticies\n",
    "                 characteristic_path_length = True, #mean of distances to other vertices\n",
    "                 efficiency = True, #mean of inverse distances to other verticies\n",
    "                 local_efficiency = True, #inverse of the shortest path in the neighborhood\n",
    "                 \n",
    "                 \n",
    "                 #graph metrics\n",
    "                  graph_characteristic_path_length = True, #mean of the local CPLs\n",
    "                  graph_global_efficiency = True, #mean of the efficiencies\n",
    "                  graph_local_efficiency = True, #mean of the local efficiencies\n",
    "                  graph_clustering_coefficient = True, #mean clustering coefficient\n",
    "                  graph_density=True, #weighted, normed by n*(n-1)\n",
    "                  graph_assortativity=True, #weighted degrees are used within a regular formula\n",
    "                  graph_assortativity_sporns=True, #weights are used as coefficients in addition to weighted degrees\n",
    "                                                   #as in Rubinov & Sporns (2010)\n",
    "                  graph_max_clique=True, #returns two features: maximal sum of weights of the largest cliques and\n",
    "                                         #mean sum of weights of the cliques of maximal size\n",
    "                  graph_transitivity=True, #note that it differs from the graph clustering coefficient,\n",
    "                                           #see Rubinov & Sporns (2010) for firmulas\n",
    "                  graph_diameter=True, #maximal eccenticity\n",
    "                  graph_radius=True, #produces three features: radius, number of central vertices and \n",
    "                               #the index of the central vertex (if one) or NA (if several)\n",
    "                  graph_alg_connectivity=True, #second-smallest eigenvalue of the Laplacian matrix\n",
    "                  graph_freeman_degree=True,#Freeman degree centralization\n",
    "                  graph_freeman_betweenness=True,#Freeman betweenness centralization\n",
    "                  graph_freeman_closeness=True,#Freeman closeness centralization\n",
    "                  graph_freeman_eigenvector=True #Freeman eigenvector centralization\n",
    "                  ):\n",
    "    \n",
    "    \n",
    "    degrees = [] # node degrees\n",
    "    neighb_deg = [] # average degree of the neighborhood of each node as in Barrat (2004)\n",
    "    neighb_deg_w = [] # average degree of the neighborhood of each node as in our ITaS paper\n",
    "    closen = [] # closeness centrality of each node\n",
    "    betw = [] # betweenness centralities\n",
    "    eigen_c = [] # eigenvector centralities\n",
    "    triang = [] # number of triangles for each node\n",
    "    clust = [] # clustering coefficients\n",
    "    ecc = [] #eccentricity\n",
    "    cpl = [] #characteristic path length\n",
    "    eff = [] #node-level global efficiency\n",
    "    leff = [] #node-level local efficiency\n",
    "    leff2 = [] #node-level local efficiency by Rubinov & Sporns (2010)\n",
    "    \n",
    "    gcpl = [] #for graph CPL\n",
    "    ggeff = [] #for graph global efficiency\n",
    "    gleff = [] #for graph local efficiency\n",
    "    gleff2 = [] #for graph local efficiency by Rubinov & Sporns (2010)\n",
    "    gcc = [] #for graph cluctering coefficient\n",
    "    gdens=[] #for graph density\n",
    "    gassort=[] #for graph assortativity\n",
    "    gassortsp = [] #for full graph assortativity (Sporns)\n",
    "    gmaxmax_cl=[] #for maximal sum of weights of maximal clique\n",
    "    gmeanmax_cl=[] #for mean sum of weights of all maximal cliques\n",
    "    gtrans=[] #for transitivity\n",
    "    gdiam=[] #for diameter\n",
    "    grad=[] #for radius\n",
    "    gn_rad = [] #for the number of central nodes\n",
    "    garg_rad = [] #for the index of central node (if the only one)\n",
    "    galg_con = [] #for graph algebraic connectivity\n",
    "    gfrdeg = [] #for Freeman degree centralization\n",
    "    gfrbetw = [] #for Freeman betweenness centralization\n",
    "    gfrclos = [] #for Freeman closeness centralization\n",
    "    gfreig = [] #for Freeman eigenvector centralization\n",
    "    \n",
    "    \n",
    "    N = len(list_of_matrices)\n",
    "    n_nodes = list_of_matrices[0].shape[0]\n",
    "    \n",
    "    star_degree = float((n_nodes - 1)*(n_nodes - 2))\n",
    "    star_closeness = star_degree/float(2*n_nodes - 3)\n",
    "    star_betweenness = float(n_nodes - 1)\n",
    "    single_eigenvector = float(n_nodes - 2)\n",
    "        \n",
    "    graph_features=[]\n",
    "    descr_features=[]\n",
    "    names_features = []\n",
    "                                                                    \n",
    "    for matrix in range(0, N):\n",
    "        A = list_of_matrices[matrix] #select an adjacency matrix\n",
    "        A_inv = 1./A #matrix with inverse weights, will be used to compute shortest path lengths \n",
    "                     #and the metrics that are based on them\n",
    "        G = ig.Graph.Weighted_Adjacency(list(A), mode=\"UNDIRECTED\", attr=\"weight\", loops=False)\n",
    "        G_inv = ig.Graph.Weighted_Adjacency(list(A_inv), mode=\"UNDIRECTED\", attr=\"weight\", loops=False)\n",
    "        Gnx = nx.from_numpy_matrix(A) #used previously\n",
    "        \n",
    "        if list_of_shortest_path_length_matrices == None: #check if matrices of shortest path length are provided\n",
    "            SPL = scipy.sparse.csgraph.dijkstra(A_inv, directed=False, unweighted=False)\n",
    "        else:\n",
    "            SPL = list_of_shortest_path_length_matrices[matrix] #select the respective matrix of shortest path lengths\n",
    "        \n",
    "        inv_SPL_with_inf = 1./SPL #matrix of inverse path lengths, with inf on the main diagonal \n",
    "                                  #and numbers elsewhere\n",
    "        inv_SPL_with_nan = inv_SPL_with_inf.copy()\n",
    "        inv_SPL_with_nan[np.isinf(inv_SPL_with_inf)]=np.nan #infs are replaced by nans\n",
    "                                            #this is done to be able to ignore them when computing means\n",
    "                                            #we only need raw means of the off-diagonal elements\n",
    "        sum_distances_vector = np.sum(SPL, 1) #vector of sum distances from a node to other nodes\n",
    "        degrees_vector=np.sum(A, 1) #vector of weighted node degrees\n",
    "        deg_by_deg_minus_one = np.multiply(degrees_vector, (degrees_vector - 1))\n",
    "        non_weighted_degrees = np.array(G.degree())\n",
    "        non_weighted_deg_by_deg_minus_one = np.multiply(non_weighted_degrees, (non_weighted_degrees - 1))\n",
    "        \n",
    "        if degree==True:\n",
    "            degrees.append(list(degrees_vector))   \n",
    "        if neighborhood_degree==True:\n",
    "            neigh = np.divide(np.dot(non_weighted_degrees.reshape(1,n_nodes), A).reshape(n_nodes,), \n",
    "                              np.array(degrees_vector, dtype = float)) #as in nx: Barrat et al. (2004)\n",
    "            neigh_w = np.divide(np.dot(degrees_vector.reshape(1,n_nodes), A).reshape(n_nodes,), \n",
    "                              np.array(degrees_vector, dtype = float)) #as in our ITaS paper\n",
    "            neighb_deg.append(neigh)\n",
    "            neighb_deg_w.append(neigh_w)\n",
    "        if closeness_centrality==True:\n",
    "            cl_c = float(n_nodes - 1)/sum_distances_vector #computed directly from SPL matrix, NORMED by (n-1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
    "            closen.append(list(cl_c))\n",
    "        if betweenness_centrality==True:\n",
    "            btw=np.array(G_inv.betweenness(weights='weight', directed=False))*2./((n_nodes-1)*(n_nodes-2))\n",
    "                                                            #Note that it can be problematic for very large graphs\n",
    "                                                            #because of the default nobigint=True option\n",
    "                                                            #normed by multiplying by 2/(n-1)(n-2)\n",
    "            betw.append(btw)\n",
    "        if eigenvector_centrality==True:\n",
    "            eigc = G.eigenvector_centrality(weights='weight', directed=False) #default scale=True is used\n",
    "                                                 #normalize the centralities so the largest one will always be 1\n",
    "            eigen_c.append(eigc)\n",
    "        if clustering_coefficient==True:\n",
    "            clst_geommean =nx.clustering(Gnx, weight='weight').values() #as in nx, Saramäki et al. (2007)\n",
    "                                                                        #and also in Rubinov & Sporns (2010)\n",
    "            clust.append(clst_geommean)  \n",
    "        if triangles==True:\n",
    "            tr = np.multiply(np.array(clst_geommean), np.array(non_weighted_deg_by_deg_minus_one, dtype = float))/2. \n",
    "                        #weighted triangles are restored from CCs, normed by non-weighted k*(k-1)\n",
    "            triang.append(list(tr))\n",
    "        if eccentricity==True:\n",
    "            eccentricities = np.max(SPL, 1) #importantly: used for diameter and radius         \n",
    "            ecc.append(list(eccentricities))\n",
    "        if characteristic_path_length==True:\n",
    "            cpls = sum_distances_vector/float(n_nodes - 1) #this was simply the inverse of the closeness \n",
    "                           #centrality (and vice versa) when closeness centralities are normed by (n-1)\n",
    "            cpl.append(list(cpls))    \n",
    "        if efficiency==True:\n",
    "            efs = np.nanmean(inv_SPL_with_nan, 1) #ignores nans on the main diagonal and thus divides by (n-1)          \n",
    "            eff.append(list(efs))\n",
    "        if local_efficiency==True: #might be NOT computationally efficient\n",
    "            lefs_num = []\n",
    "            lefs1 = []\n",
    "            for node in range(0, n_nodes):\n",
    "                neighb_list = G.neighbors(node)\n",
    "                subgraph_size = len(neighb_list)\n",
    "                if subgraph_size < 2:\n",
    "                    lefs_num.append(float(0))\n",
    "                    lefs1.append(float(0))\n",
    "                else:\n",
    "                    G_subgraph = G_inv.induced_subgraph(neighb_list)\n",
    "                    SPL_subgraph = np.array(G_subgraph.shortest_paths(weights='weight'))\n",
    "                    inv_SPL_subgraph = 1./SPL_subgraph #diagonal elements were 0, became infs\n",
    "                    inv_SPL_subgraph[np.isinf(inv_SPL_subgraph)]=0 #replace diagonal elements by zeros so that \n",
    "                                                                   #they do not add to sums\n",
    "                    lefs1.append(np.sum(inv_SPL_subgraph)/(2*float(non_weighted_deg_by_deg_minus_one[node])))\n",
    "                    leff_values = []\n",
    "                    for i in range(0, subgraph_size):\n",
    "                        for j in range(i+1, subgraph_size):\n",
    "                            leff_value = np.power(A[node, neighb_list[i]]*A[node, neighb_list[j]]*inv_SPL_subgraph[i,j], 1./3.)\n",
    "                            leff_values.append(leff_value)\n",
    "                    lefs_num.append(np.sum(leff_values))\n",
    "            lefs = np.divide(lefs_num, non_weighted_deg_by_deg_minus_one)\n",
    "            lefs[np.where(np.array(lefs_num)==0)[0]]=0.\n",
    "                        \n",
    "            leff.append(lefs1)                     \n",
    "            leff2.append(list(lefs))    \n",
    "        \n",
    "        #Graph-level metrics:\n",
    "        if graph_characteristic_path_length ==True:\n",
    "            gcpl.append(np.mean(cpls))\n",
    "        if graph_global_efficiency == True:\n",
    "            ggeff.append(np.mean(efs))\n",
    "        if graph_local_efficiency ==True:\n",
    "            gleff.append(np.mean(lefs1)) \n",
    "            gleff2.append(np.mean(lefs)) #means are here intentionally (looks like mistake in Rubinov & Sporns, 2010)\n",
    "        if graph_clustering_coefficient == True:\n",
    "            gcc.append(np.mean(clst_geommean))\n",
    "        if graph_density == True:\n",
    "            dens_G = np.sum(A)/float(n_nodes*(n_nodes-1)) #compute weighted density as the sum of all weights \n",
    "                                                        #divided by the possible number of edges\n",
    "            gdens.append(dens_G)\n",
    "        if graph_assortativity ==True: \n",
    "            assort = G.assortativity(types1 = degrees_vector, directed = False)\n",
    "            if np.isnan(assort)==True:\n",
    "                gassort.append(0)\n",
    "            else:\n",
    "                gassort.append(assort)\n",
    "        if graph_assortativity_sporns == True: \n",
    "            degrees_squared = np.power(degrees_vector, 2)\n",
    "            max_w = np.max(A)\n",
    "            max_w_cube = np.power(max_w, 3)\n",
    "            max_w_sq = np.power(max_w, 2)\n",
    "            inv_l = 2./len(np.nonzero(A)[0])\n",
    "            deg_mult = np.zeros((n_nodes, n_nodes))\n",
    "            deg_sum = np.zeros((n_nodes, n_nodes))\n",
    "            deg_sq_sum = np.zeros((n_nodes, n_nodes))\n",
    "            for i in range(0, n_nodes):\n",
    "                for j in range(i+1, n_nodes):\n",
    "                    deg_mult[i,j] = degrees_vector[i]*degrees_vector[j]\n",
    "                    deg_mult[j,i] = deg_mult[i,j]\n",
    "                    deg_sum[i,j] = degrees_vector[i] + degrees_vector[j]\n",
    "                    deg_sum[j,i] = deg_sum[i,j]\n",
    "                    deg_sq_sum[i,j] = degrees_squared[i] + degrees_squared[j]\n",
    "                    deg_sq_sum[j,i] = deg_sq_sum[i,j]\n",
    "            assortsp1 = inv_l * np.sum(np.multiply(A, deg_mult))/float(2.*max_w_cube)\n",
    "            assortsp2 = np.power((inv_l*(1./(4.*max_w_sq)))*np.sum(np.multiply(A, deg_sum)),2)\n",
    "            assortsp3 = (inv_l*(1./(4.*max_w_cube)))*np.sum(np.multiply(A, deg_sq_sum))\n",
    "            assortsp = (assortsp1 - assortsp2)/float(assortsp3 - assortsp2)\n",
    "            gassortsp.append(assortsp)\n",
    "        if graph_max_clique == True: \n",
    "            max_cliques = G.largest_cliques()\n",
    "            weights_of_max_cl = []\n",
    "            for clique in max_cliques:\n",
    "                weights_of_max_cl.append(np.sum(G.induced_subgraph(clique).strength(weights='weight'))/2.)\n",
    "            gmaxmax_cl.append(np.max(weights_of_max_cl))\n",
    "            gmeanmax_cl.append(np.mean(weights_of_max_cl))\n",
    "        if graph_transitivity == True: \n",
    "            trans = float(2*np.sum(tr))/float(np.sum(non_weighted_deg_by_deg_minus_one))\n",
    "            gtrans.append(trans)\n",
    "        if graph_diameter==True:\n",
    "            gdiam.append(np.max(eccentricities))\n",
    "        if graph_radius == True:       \n",
    "            min_ecc = np.min(eccentricities) \n",
    "            ind_rad = np.where(eccentricities==min_ecc)[0]\n",
    "            n_rad = len(list(ind_rad))\n",
    "            grad.append(min_ecc)\n",
    "            gn_rad.append(n_rad)\n",
    "            if n_rad==1:\n",
    "                garg_rad.append(ind_rad[0])\n",
    "            else:\n",
    "                garg_rad.append(np.nan)\n",
    "        if graph_alg_connectivity == True:\n",
    "            sparse_lapl = scipy.sparse.csr_matrix(np.array(G.laplacian(weights='weight')))\n",
    "            second_sm_eigv = scipy.sparse.linalg.eigsh(sparse_lapl, k=2, which='SM', maxiter=100000, return_eigenvectors = False)[0]\n",
    "            galg_con.append(second_sm_eigv)\n",
    "        if graph_freeman_degree == True:\n",
    "            frdeg_num = np.sum(np.max(degrees_vector) - degrees_vector)\n",
    "            gfrdeg.append(frdeg_num/star_degree)\n",
    "        if graph_freeman_betweenness == True:\n",
    "            frbetw_num = np.sum(np.max(btw) - btw)\n",
    "            gfrbetw.append(frbetw_num/star_betweenness)\n",
    "        if graph_freeman_closeness == True:\n",
    "            frclos_num = np.sum(np.max(cl_c) - cl_c)\n",
    "            gfrclos.append(frclos_num/star_closeness)\n",
    "        if graph_freeman_eigenvector == True:\n",
    "            freig_num = np.sum(np.max(eigc) - eigc)\n",
    "            gfreig.append(freig_num/single_eigenvector)\n",
    "                \n",
    "    if graph_characteristic_path_length == True:\n",
    "        graph_features.append(gcpl)\n",
    "        descr_features.append('graph characteristic path length (mean of the characteristic path lenghts)')\n",
    "        names_features.append('graph_characteristic_path_length')\n",
    "    if graph_global_efficiency == True:\n",
    "        graph_features.append(ggeff)\n",
    "        descr_features.append('graph global efficiency (mean of the node-level global efficiencies)')\n",
    "        names_features.append('graph_global_efficiency')\n",
    "    if graph_local_efficiency == True:\n",
    "        graph_features.append(gleff)\n",
    "        descr_features.append('graph local efficiency (mean of the node-level local efficiencies)')        \n",
    "        names_features.append('graph_local_efficiency')\n",
    "        graph_features.append(gleff2)\n",
    "        descr_features.append('graph local efficiency (mean of the node-level local efficiencies by Sporns)')        \n",
    "        names_features.append('graph_local_efficiency_sporns')\n",
    "    if graph_clustering_coefficient == True:\n",
    "        graph_features.append(gcc)\n",
    "        descr_features.append('graph clustering coefficient (mean of the clustering coefficients)')        \n",
    "        names_features.append('graph_clustering_coefficient')\n",
    "    if graph_density == True:\n",
    "        graph_features.append(gdens)\n",
    "        descr_features.append('graph weighted density (normed by maximal possible density of unweighted graph of the same size)')\n",
    "        names_features.append('graph_density')\n",
    "    if graph_assortativity == True: \n",
    "        graph_features.append(gassort)\n",
    "        descr_features.append('graph assortativity by weighted degree')\n",
    "        names_features.append('graph_assortativity')\n",
    "    if graph_assortativity_sporns==True: \n",
    "        graph_features.append(gassortsp)\n",
    "        descr_features.append('graph weighted assortativity as described in Rubinov & Sporns (2010)')\n",
    "        names_features.append('graph_assortativity_sporns')\n",
    "    if graph_max_clique == True: \n",
    "        graph_features.append(gmaxmax_cl)\n",
    "        descr_features.append('maximal sum of weights of the cliques of the largest size')\n",
    "        names_features.append('max_weights_sum_largest_cliques')\n",
    "        graph_features.append(gmeanmax_cl)\n",
    "        descr_features.append('mean sum of weights of the cliques of the largest size')\n",
    "        names_features.append('mean_weights_sum_largest_cliques')\n",
    "    if graph_transitivity == True: \n",
    "        graph_features.append(gtrans)\n",
    "        descr_features.append('graph transitivity')\n",
    "        names_features.append('graph_transitivity')\n",
    "    if graph_diameter == True:\n",
    "        graph_features.append(gdiam) \n",
    "        descr_features.append('graph weighted diameter')\n",
    "        names_features.append('graph_diameter')\n",
    "    if graph_radius==True:       \n",
    "        graph_features.append(grad)              \n",
    "        descr_features.append('graph weighted radius')\n",
    "        names_features.append('graph_radius')\n",
    "        graph_features.append(gn_rad)              \n",
    "        descr_features.append('graph number of centers')\n",
    "        names_features.append('graph_number_of_centers')\n",
    "        graph_features.append(garg_rad)              \n",
    "        descr_features.append('graph center (if a single vertex)')\n",
    "        names_features.append('graph_center')\n",
    "    if graph_alg_connectivity == True:       \n",
    "        graph_features.append(galg_con)              \n",
    "        descr_features.append('graph algebraic connectivity (second-smallest eigenvalue of the Laplacian matrix)')\n",
    "        names_features.append('graph_algebraic_connectivity')\n",
    "    if graph_freeman_degree == True:       \n",
    "        graph_features.append(gfrdeg)              \n",
    "        descr_features.append('graph Freeman degree centralization (normed by unweighted star centralization)')\n",
    "        names_features.append('graph_degree_centralization')\n",
    "    if graph_freeman_betweenness == True:       \n",
    "        graph_features.append(gfrbetw)              \n",
    "        descr_features.append('graph Freeman betweenness centralization (normed by unweighted star centralization)')\n",
    "        names_features.append('graph_betweenness_centralization')\n",
    "    if graph_freeman_closeness == True:       \n",
    "        graph_features.append(gfrclos)              \n",
    "        descr_features.append('graph Freeman closeness centralization (normed by unweighted star centralization)')\n",
    "        names_features.append('graph_closeness_centralization')\n",
    "    if graph_freeman_eigenvector == True:       \n",
    "        graph_features.append(gfreig)              \n",
    "        descr_features.append('graph Freeman eigenvector centralization (normed by unweighted single-edge graph centralization)')\n",
    "        names_features.append('graph_eigenvector_centralization')    \n",
    "    \n",
    "    graph_features_array = np.transpose(np.array(graph_features))\n",
    "    \n",
    "    if degree==True:\n",
    "        node_features = np.array(degrees)\n",
    "        for n in range(0, n_nodes):\n",
    "            descr_features.append('weighted degree node '+str(n))\n",
    "            names_features.append('degree_node_'+str(n))\n",
    "    if  neighborhood_degree == True:\n",
    "        node_features = np.hstack((node_features, np.array(neighb_deg)))\n",
    "        node_features = np.hstack((node_features, np.array(neighb_deg_w)))\n",
    "        for n in range(0, n_nodes):\n",
    "            descr_features.append('Barrat average neighborhood weighted degree node '+ str(n))\n",
    "            names_features.append('barrat_neighborhood_degree_node_'+ str(n))\n",
    "        for n in range(0, n_nodes):\n",
    "            descr_features.append('Our average neighborhood weighted degree node '+ str(n))\n",
    "            names_features.append('neighborhood_degree_node_'+ str(n))\n",
    "    if  closeness_centrality==True:              \n",
    "        node_features = np.hstack((node_features, np.array(closen)))\n",
    "        for n in range(0, n_nodes):\n",
    "            descr_features.append('non-normed closeness centrality node ' + str(n))\n",
    "            names_features.append('closeness_node_' + str(n))\n",
    "    if  betweenness_centrality==True:              \n",
    "        node_features = np.hstack((node_features, np.array(betw)))\n",
    "        for n in range(0, n_nodes):\n",
    "            descr_features.append('non-normed betweenness centrality node ' + str(n))\n",
    "            names_features.append('betweenness_node_' + str(n))     \n",
    "    if  eigenvector_centrality == True:              \n",
    "        node_features = np.hstack((node_features, np.array(eigen_c)))\n",
    "        for n in range(0, n_nodes):\n",
    "            descr_features.append('normed (max 1) eigenvector centrality node '+ str(n))\n",
    "            names_features.append('eigenvector_centrality_node_'+ str(n))  \n",
    "    if  clustering_coefficient==True:              \n",
    "        node_features = np.hstack((node_features, np.array(clust)))\n",
    "        for n in range(0, n_nodes):\n",
    "            descr_features.append('clustering coefficient node ' + str(n))\n",
    "            names_features.append('clustering_coefficient_node_' + str(n))  \n",
    "    if  triangles==True:              \n",
    "        node_features = np.hstack((node_features, np.array(triang)))\n",
    "        for n in range(0, n_nodes):\n",
    "            descr_features.append('weighted triangles node ' + str(n))\n",
    "            names_features.append('triangles_node_' + str(n))\n",
    "    if  eccentricity==True:              \n",
    "        node_features = np.hstack((node_features, np.array(ecc)))\n",
    "        for n in range(0, n_nodes):\n",
    "            descr_features.append('eccentricity node ' + str(n))\n",
    "            names_features.append('eccentricity_node_' + str(n))\n",
    "    if  characteristic_path_length==True:              \n",
    "        node_features = np.hstack((node_features, np.array(cpl)))\n",
    "        for n in range(0, n_nodes):\n",
    "            descr_features.append('characteristic path length node ' + str(n))\n",
    "            names_features.append('characteristic_path_length_node_' + str(n))                              \n",
    "    if  efficiency==True:              \n",
    "        node_features = np.hstack((node_features, np.array(eff)))\n",
    "        for n in range(0, n_nodes):\n",
    "            descr_features.append('efficiency node ' + str(n))\n",
    "            names_features.append('efficiency_node_' + str(n))                                  \n",
    "    if  local_efficiency==True:              \n",
    "        node_features = np.hstack((node_features, np.array(leff)))\n",
    "        for n in range(0, n_nodes):\n",
    "            descr_features.append('local efficiency node ' + str(n))\n",
    "            names_features.append('local_efficiency_node_' + str(n))                                  \n",
    "        node_features = np.hstack((node_features, np.array(leff2)))\n",
    "        for n in range(0, n_nodes):\n",
    "            descr_features.append('local efficiency by Rubinov & Sporns node ' + str(n))\n",
    "            names_features.append('local_efficiency_sporns_node_' + str(n))                                  \n",
    "    fin_features = np.hstack((graph_features_array, node_features))\n",
    "    fin_features_dataframe = pd.DataFrame(fin_features, columns = names_features)                              \n",
    "    \n",
    "    return (fin_features_dataframe, descr_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовим данные и обучим модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = 'Data/dti/'\n",
    "data = get_autism(path)\n",
    "data = matrix_eig(data, k = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_new = data['X_low']\n",
    "X = data['X']\n",
    "y = data['y'] \n",
    "print X.shape, X_new.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fin_feat, descr_feat = symm_metrics(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(X_new.shape[0]):\n",
    "    X_new[i][X_new[i] < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fin_feat_new, descr_feat_new = symm_metrics(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fin_feat_new.to_csv(\"metric-low.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fin_feat.to_csv(\"metric-orig.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
